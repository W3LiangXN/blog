# 大文件上传

在日常的工作中，有时候会遇到大文件上传的问题, 如上传入库比较大的Excel表格数据、上传影音文件等。如果文件体积比较大，或者网络条件不好时，上传的时间会比较长（要传输更多的报文，丢包重传的概率也更大），用户不能刷新页面，只能耐心等待请求完成。为了更好解决问题, 就需要我们对大文件的上传进行处理。

### 整体思路
#### 前端
前端大文件，核心是利用 Blob.prototype.slice 方法，和数组的 slice 方法相似，调用的 slice 方法可以返回原文件的某个切片

这样我们就可以根据预先设置好的切片最大数量将文件切分为一个个切片，然后借助 http 的可并发性，同时上传多个切片，这样从原本传一个大文件，变成了同时传多个小的文件切片，可以大大减少上传时间

另外由于是并发，传输到服务端的顺序可能会发生变化，所以我们还需要给每个切片记录顺序

#### 服务端
服务端需要负责接受这些切片，并在接收到所有切片后合并切片

这里又引伸出两个问题
 1. 何时合并切片，即切片什么时候传输完成
 2. 如何合并切片

第一个问题需要前端进行配合，前端在每个切片中都携带切片最大数量的信息，当服务端接受到这个数量的切片时自动合并，也可以额外发一个请求主动通知服务端进行切片的合并

第二个问题，具体如何合并切片呢？这里可以使用 nodejs 的 读写流（readStream/writeStream），将所有切片的流传输到最终文件的流里

### 前端代码
#### html
```html
<input id="fileInput" type="file">
<br />
<br />
<button id="uploadBtn">上传文件</button>
```
简单的一个 input 标签来接收文件, button 标签控制来上传

#### javascript 代码

##### 封装请求
考虑到通用性，这里没有用第三方的请求库，而是用原生 XMLHttpRequest 做一层简单的封装来发请求

```js
const request = ({ url, method = "post", data, headers = {} }) => {
  return new Promise((resolve, reject) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    Object.keys(headers).forEach(key =>
      xhr.setRequestHeader(key, headers[key])
    )
    xhr.send(data)
    xhr.onload = e => {
      resolve({
        data: e.target.response
      })
    }
  })
}
```

##### 拆分切片以及上传

```js
const fileInputDom = document.querySelector('#fileInput')
const uploadBtnDom = document.querySelector('#uploadBtn')
let needUploadFile = null // 需要上传的文件
const SIZE = 5 * 1024 * 1024 // 切片大小 5M一块
let chunkDataList = [] // 保存切片数组

// 生成文件切片
const createFileChunk = (file, size = SIZE) => {
  const fileChunkList = []
  let cur = 0
  while (cur < file.size) {
    fileChunkList.push({ file: file.slice(cur, cur + size) })
    cur += size
  }
  return fileChunkList
}

// 上传切片
const uploadChunks = async () => {
  const requestList = chunkDataList.map(({ chunk, hash }) => {
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", needUploadFile.name)
      return { formData }
    })
    .map(({ formData }) =>
      request({
        url: "http://localhost:7400/upload",
        data: formData
      })
    )
  await Promise.all(requestList) // 并发切片
}

// 监听 input 变化
const onChange = (event) => {
  const [file] = event.target.files
  if (!file) return
  needUploadFile = file
}

// 处理文件并上传
const handleUploadFile = () => {
  if (!needUploadFile) {
    alert('请上传文件')
    return
  }
  const fileChunkList = createFileChunk(needUploadFile)
  chunkDataList = fileChunkList.map(({ file }, index) => ({
    chunk: file, // 文件
    hash: needUploadFile.name + "-" + index // 文件名 + 数组下标
  }))
  uploadChunks()
}


fileInputDom.addEventListener('change', onChange)
uploadBtnDom.addEventListener('click', handleUploadFile)
```

当点击上传按钮时，调用 **createFileChunk** 将文件切片，切片数量通过文件大小控制，这里设置 5MB，也就是说 50 MB 的文件会被分成 10 个切片

**createFileChunk** 内使用 while 循环和 slice 方法将切片放入 fileChunkList 数组中返回

在生成文件切片时，需要给每个切片一个标识作为 hash，这里暂时使用 **文件名 + 下标**，这样后端可以知道当前切片是第几个切片，用于之后的合并切片

随后调用 **uploadChunks** 上传所有的文件切片，将文件切片，切片 hash，以及文件名放入 FormData 中，再调用上一步的 request 函数返回一个 promise，最后调用 Promise.all 并发上传所有的切片

### 服务端代码

简单使用 http 模块搭建服务端

```js
const http = require("http");
const server = http.createServer();

server.on("request", async (req, res) => {
  res.setHeader("Access-Control-Allow-Origin", "*")
  res.setHeader("Access-Control-Allow-Headers", "*")
  if (req.method === "OPTIONS") {
    res.status = 200
    res.end()
    return
  }
})

server.listen(7400, () => console.log("正在监听 7400 端口"))
```

#### 接受切片

使用 multiparty 包处理前端传来的 FormData

在 multiparty.parse 的回调中，files 参数保存了 FormData 中文件，fields 参数保存了 FormData 中非文件的字段

```js
server.on("request", async (req, res) => {
  res.setHeader('Access-Control-Allow-Origin', '*')
  res.setHeader('Access-Control-Allow-Headers', '*')

  // post 预检请求
  if (req.method === "OPTIONS") {
    res.status = 200
    res.end()
    return
  }

  if (req.url === "/upload") {
    const multipart = new multiparty.Form()
    multipart.parse(req, async (err, fields, files) => {
      if (err) return

      const [hash] = fields.hash
      const [filename] = fields.filename
      const [chunk] = files.chunk
      const chunkDir = path.resolve(UPLOAD_DIR, filename.split('.')[0])
      
      // 切片目录不存在，创建切片目录
      if (!fse.existsSync(chunkDir)) {
        await fse.mkdirs(chunkDir)
      }

      await fse.move(chunk.path, `${chunkDir}/${hash}`)
      res.end("received file chunk")
    })
  }
})

server.listen(7400, () => console.log("正在监听 7400 端口"))
```
![images](https://user-gold-cdn.xitu.io/2020/1/11/16f905fb6a626f47?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

查看 multiparty 处理后的 chunk 对象，path 是存储临时文件的路径，size 是临时文件大小，使用 fse.move 移动临时文件，即移动文件切片

到这里我们就可以看到 当上传大文件的时候, 已经被分成了多个切片且保存在了服务端, 接下来我们就需要把切片合并。

首先我们在前端的代码加上了合并的请求给服务端

```js
// 上传切片
const uploadChunks = async () => {
  // doing something
  await Promise.all(requestList) // 并发切片
  + await mergeRequest() // 合并切片
}
```

```js
const mergeRequest = async () => {
  await request({
    url: "http://localhost:7400/merge",
    headers: {
      "content-type": "application/json"
    },
    data: JSON.stringify({
      filename: needUploadFile.name
    })
  })
}
```

服务端代码

```js
const http = require('http')
const path = require('path')
const fse = require('fs-extra')
const multiparty = require('multiparty')

const server = http.createServer()
const UPLOAD_DIR = path.resolve(__dirname, '..', 'target') // 文件存储目录

+ // 解析 post 请求 data 数据
+ const resolvePost = req => {
+  return new Promise((resolve, reject) => {
+    let chunk = ''
+    req.on('data', data => {
+      chunk += data
+    })
+    req.on("end", () => {
+      resolve(JSON.parse(chunk))
+    })
+  })
+ }

+ const pipeStream = (path, writeStream) => {
+  return new Promise((resolve, reject) => {
+    const readStream = fse.createReadStream(path)
+    readStream.on("end", () => {
+      // 删除临时存储的切片文件文件
+      fse.unlinkSync(path)
+      resolve()
+    })
+    readStream.pipe(writeStream)
+  })
+ }

+ // 合并切片
+ const mergeFileChunk = async (filePath, filename, size) => {
+  const chunkDir = path.resolve(UPLOAD_DIR, filename)
+  const chunkPaths = await fse.readdir(chunkDir)
+  // 根据切片下标进行排序
+  // 否则直接读取目录的获得的顺序可能会错乱
+  chunkPaths.sort((a, b) => a.split("-")[1] - b.split("-")[1])
+  await Promise.all(
+    chunkPaths.map((chunkPath, index) =>
+      pipeStream(
+        path.resolve(chunkDir, chunkPath),
+        // 指定位置创建可写流
+        fse.createWriteStream(filePath, {
+          start: index * size,
+          end: (index + 1) * size
+        })
+      )
+    )
+  )
+  fse.rmdirSync(chunkDir)
+ }

server.on("request", async (req, res) => {
  res.setHeader('Access-Control-Allow-Origin', '*')
  res.setHeader('Access-Control-Allow-Headers', '*')

  // post 预检请求
  if (req.method === "OPTIONS") {
    res.status = 200
    res.end()
    return
  }

  if (req.url === "/upload") {
    const multipart = new multiparty.Form()
    multipart.parse(req, async (err, fields, files) => {
      if (err) return
      // files:  { chunk: [{ fieldName: 'chunk', originalFilename: 'blob', path: '/var/folders/25/f2c2w6k923z4y7tfjx98vvy40000gn/T/lgs_eL5ZEDm81uI35N_4oCVK', headers: [Object], size: 5242880 } ] }
      // fields: { hash: [ '周杰伦 稻香.mp4-1' ], filename: [ '周杰伦 稻香.mp4' ] }

      const [hash] = fields.hash
      const [filename] = fields.filename
      const [chunk] = files.chunk
      const chunkDir = path.resolve(UPLOAD_DIR, filename.split('.')[0])
      
      // 切片目录不存在，创建切片目录
      if (!fse.existsSync(chunkDir)) {
        await fse.mkdirs(chunkDir)
      }

      await fse.move(chunk.path, `${chunkDir}/${hash}`)
      res.end("received file chunk")
    })
  }

+  if (req.url === "/merge") {
+    const data = await resolvePost(req)
+    const { filename, size } = data
+    const filePath = path.resolve(UPLOAD_DIR, filename)
+    await mergeFileChunk(filePath, filename.split('.')[0], size)
+    res.end(
+      JSON.stringify({
+        code: 0,
+        message: "file merged success"
+      })
+    )
+    return
+  }

})

server.listen(7400, () => console.log("正在监听 7400 端口"))

```

由于前端在发送合并请求时会携带文件名，服务端根据文件名可以找到上一步创建的切片文件夹

接着使用 fs.createWriteStream 创建一个可写流，可写流文件名就是**切片文件夹名 + 后缀名**组合而成

随后遍历整个切片文件夹，将切片通过 fs.createReadStream 创建可读流，传输合并到目标文件中
值得注意的是每次可读流都会传输到可写流的指定位置，这是通过 createWriteStream 的第二个参数 start/end 控制的，目的是能够并发合并多个可读流到可写流中，这样即使流的顺序不同也能传输到正确的位置，所以这里还需要让前端在请求的时候多提供一个 size 参数

```js
const mergeRequest = async () => {
  await request({
    url: "http://localhost:7400/merge",
    headers: {
      "content-type": "application/json"
    },
    data: JSON.stringify({
+      size: SIZE,
       filename: needUploadFile.name
    })
  })
}
```

其实也可以等上一个切片合并完后再合并下个切片，这样就不需要指定位置，但传输速度会降低，所以使用了并发合并的手段，接着只要保证每次合并完成后删除这个切片，等所有切片都合并完毕后最后删除切片文件夹即可


> 参考文档: https://juejin.cn/post/6844904046436843527